This is a draft article and a response to the post [Mobile Web](http://lilly.tumblr.com/post/38236111289/the-mobile-web) from John Lilly.

## Mobile network index is hand written

John's point, that cauth my attention, was his recall for the transformative web: "the utility of being able to do things in a networked way, with other people, connected all the time, and being able to mash up stuff from one place into another".

And that transformative web was a network in development at a incredible speed. Netscape founder, Jim Barksdale at the [luncheon event at the National Press Club on Netscape](http://www.youtube.com/watch?v=wgufqTgsPsI), asked many times "what is going on here?" — why things were scaling so fast. Jim cited [Melcalfe's law](http://en.wikipedia.org/wiki/Metcalfe%27s_law) and that it works that way when you have ubiquity: "fast networks grows once people believe — I gotta be in the network or I am gonna be left out." And he jokes about a salesman trying to sell the first telephone in a world with no other phones. 

But going for this networked and mashable web seems to be a difficult path to make an analysis. I find myself sometimes looking for one answer like trying to describe a one wave with ups and downs — and going towards my assesment that indeed we are in the down of a networked web. But perhaps this wave is not a wave but many at the same time. There is certainly recognition from users — they undersood cellphones and they understood the magic of the web so they understand that you can get an online device that comes with an online and offline hybrid experience that you can query the web, maps, phones, friends, updates, and do many sorts of pushs. The index is manual right? I mean the market is almost hand written, crafted — so it was the same way when it all started — let's think Yahoo! and things like Dmoz.

But then it comes the question now which is how can an index or system act to help users to map the network? If we have manually written indexes is because there is indeed something blocking our way. Now the tricky thing is that the story zsticks — so there is ubiquity from the user's point of view: they respect the explanation markets are saying: you can trust the apps, the apps are ranked – it does not matter that it is a lot hand made — it might even sound better for users now. The insecure side of the Web helped everyone to live a bit worried about these things. 

Let's take a first generation of Add-ons in the Mozilla space — add-ons, a kind of first generation of web apps if you will, used to be posted in any site. Then Mozilla created the index, manually reviewed, and things became organized but yet people were able to modify the web — to exercise a bit the re-writable web — it was magic to be able to "fix a problem with the bank site" and to do things "on behalf of the users". The top add-ons in the Mozilla space were things that could change sites — things like filtering only images, fetching videos, blocking pop-ups and so on. The index is manual right? I mean the market is almost hand written, crafted — so it was the same way when it all started — let's think Yahoo! and things like Dmoz.

But then it comes the question now which is how can an index or system act to help users to map the network? If we have manually written indexes is because there is indeed something blocking our way. Now the tricky thing is that the story zsticks — so there is ubiquity from the user's point of view: they respect the explanation markets are saying: you can trust the apps, the apps are ranked – it does not matter that it is a lot hand made — it might even sound better for users now. The insecure side of the Web helped everyone to live a bit worried about these things. 

Let's take a first generation of Add-ons in the Mozilla space — add-ons, a kind of first generation of web apps if you will, used to be posted in any site. Then Mozilla created the index, manually reviewed, and things became organized but yet people were able to modify the web — to exercise a bit the re-writable web — it was magic to be able to "fix a problem with the bank site" and to do things "on behalf of the users". The top add-ons in the Mozilla space were things that could change sites — things like filtering only images, fetching videos, blocking pop-ups and so on. 

## Historical recap

Mozdev.org served well as the very first (developer recognized) index for Mozilla add-ons. It was the birthplace for ideas like the [Banner blind](http://bannerblind.mozdev.org/) an ad removal add-on. And experimental projects like [Macrozilla](http://macrozilla.mozdev.org/) a project to let developers modify page behavior running scripts on top of pages. 

But things grow and recognition and user intimacy are indicators for scaling things, so a market place in the Mozilla space was written, [Addons.mozilla.org](http://addons.mozilla.org). Now we pick add-ons written with the new Mozilla SDK and what we get there is a new story: ergonomics for developers and it's an infra-structure that can help the index to be more of a right thing under the safe, secure, and controlled approach — so a plain SDK-written add-on won't make much things like top 5% add-ons used to do — exercise the transformational web. 

Plain add-ons are more locked — in a way similar to what mobile web apps can do — they ask for permissions and the index know about it. On the mobile it's the intent model so what you get is a lot of apps asking permissions to exercise the "copy and paste" — it supports and index and the index will say that it is safer. 

So what is left to these apps are basically to deal with remote sources and the sources they can deal - most cases their trusted domains. This effect started with the Addons in the Mozilla space and it is common to see with mobile apps over the markets — apps are tunnels to a very specific rabbit hole. There is ubiquity if we ask users – they are all going towards this story. But there is no networked ubiquity so a level that a third-party index can do its work in the network — and mostly because things are not hyperlinked with a ranking model that supports it. And even worse is that many of us are worring that the ranking model is not the same anymore and that a traditional index (think Google) is not as good.

But a question that comes is how to map the relationships to restablish a web that could be ranked and mapped. And I think is worth looking why people do links to pages and what pushes them to do it. 

Hyperlinks are cool because they served as a way to help us write less — we add evidence, exampes, references and we do it inline. Going back to Netscape navigator Gold Edition — we had an editor in it. Page writters were initially from universities and they knew how to write bibliographic references — so they did in the new way. And this was a model between a written book and the spoken world. Web pages allowed us to establish something between a speech and scientific article. And the search engine was the marketplace for us to find all. 

Now intents are not hyperlinks but it certainly boosts web conversations. We have now much more videos, documents, and web artifacts but they are growing in a vertical approach. There is a certain a grid of pilars going on — a list of best TVs, a list of beautiful flowers, follow my dance steps, misheard lyrics — it's an exercise that helped us to modify things but to keep references – just like developers can write lines of code in a repository and they can look back — a pilar, a construction. From the values we have today I see recognition that won't be modified but that any person can reply to the web if they are using specific agents to this expression — web services and web apps. 

But in what layer we need to look at to build this network? Can we make web pages become apps? Or simply the generated results from apps are going to be pages with links to other resources and apps too — a mark from the editor goes along with results? It's difficult because it not one network anymore — we have many now. The grid of service pilars is like saying that everyone (all mobile users) is now using a editor — they are producting web artifacts but they are producing speciallized artifacts — an image, a statement over twitter, a map annotation. They are producing the links only, not the text around things. And the reason it works is because they are producing these links in a context — think that a twitter tag in an event is the context — the calling voice, the claim. The links are the statements, primisses that are intended to support things. This is a conversation and this is an article but the problem we have to deal now is how to "crawll" these things. And it is big because a lot of these pilars are under closed networks — think a discussion that goes on in facebook or the permissions you need to capture real-time twitter discussions. 

So we are failing to capture discussions in the pilars which prevents us from making a web between these resources. And on the other hand the editor it at our ands — and editors won't exactly comply with the crawling rules too — so think about the days Flash was going up stream — a proprietary editor with an incentive — animation — and they helped the web to be closed and indexes failed to reach out to them. Now it is a bit of that in our hands in our mobile — flashing things down the tunnel — to not say a toiled because I feel we may still find a light in there. 

